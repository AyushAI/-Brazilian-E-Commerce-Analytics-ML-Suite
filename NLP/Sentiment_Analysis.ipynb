{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b109a4b5",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "<font color=\"darkslateblue\" size=+2.5><b>Natural Language Processing</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a315dd",
   "metadata": {},
   "source": [
    "As long as we could improve our relationship with the data, the path is open to start the Natural Language Processing step to analyze the comments left on e-commerce orders. The goal is to use this as input to a `sentimental analysis` model for understanding the customer's sentiment on purchasing things online. Let's take a look on the reviews data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b4c4d0",
   "metadata": {},
   "source": [
    "<a id=\"4.1\"></a>\n",
    "<font color=\"dimgrey\" size=+2.0><b>Data Understanding</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc857740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39b72f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (40977, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Recebi bem antes do prazo estipulado.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Parab√©ns lojas lannister adorei comprar pela I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>aparelho eficiente. no site a marca do aparelh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Mas um pouco ,travando...pelo valor ta Boa.\\r\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Vendedor confi√°vel, produto ok e entrega antes...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   score                                            comment\n",
       "0      5              Recebi bem antes do prazo estipulado.\n",
       "1      5  Parab√©ns lojas lannister adorei comprar pela I...\n",
       "2      4  aparelho eficiente. no site a marca do aparelh...\n",
       "3      4    Mas um pouco ,travando...pelo valor ta Boa.\\r\\n\n",
       "4      5  Vendedor confi√°vel, produto ok e entrega antes..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Load dataset\n",
    "df = pd.read_csv(\"/home/ayush-wase/E Commerce ML/dataset/olist_order_reviews_dataset.csv\")\n",
    "\n",
    "# 2. Keep only the columns we need\n",
    "df = df[[\"review_score\", \"review_comment_message\"]]\n",
    "\n",
    "# 3. Remove rows with empty comments\n",
    "df = df.dropna()\n",
    "\n",
    "# 4. Rename columns for simplicity\n",
    "df.columns = [\"score\", \"comment\"]\n",
    "\n",
    "# 5. Reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# 6. Check data\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1affc695",
   "metadata": {},
   "source": [
    "So, we have in hands approximately 41k comments that could be used for training a sentimental analysis model. But, for this to becoming true, we have to go trough a long way of text preparation to transform the comment input into a vector that can be interpreted for a Machine Learning model. **Let's go ahead**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6126ad4a",
   "metadata": {},
   "source": [
    "<a id=\"4.2\"></a>\n",
    "<font color=\"dimgrey\" size=+2.0><b>Regular Expressions</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc897551",
   "metadata": {},
   "source": [
    "As long as we consider the global internet as the source of our comments, probably we have to deal with some HTML tags, break lines, special characteres and other content that could be part of the dataset. Let's dig a little bit more on `Regular Expressions` to search for those patterns.\n",
    "\n",
    "First of all, let's define a function that will be used for analysing the results of an applied regular expression. With whis we can validate our text pre processing in an easier way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8456f93e",
   "metadata": {},
   "source": [
    "<a id=\"4.2.1\"></a>\n",
    "<font color=\"dimgrey\" size=+1.5><b>Breakline and Carriage Return</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20c05900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12256</th>\n",
       "      <td>2</td>\n",
       "      <td>O CORREIO ME ENTREGOU SOMENTE UMA CORTINA SEND...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25586</th>\n",
       "      <td>3</td>\n",
       "      <td>Recebi parcialmente o pedido. De tr√™s rel√≥gios...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29493</th>\n",
       "      <td>5</td>\n",
       "      <td>Bom produto, r√°pida entrega! Recomendo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5118</th>\n",
       "      <td>5</td>\n",
       "      <td>Chegou tudo conforme esperado. O que mais surp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21710</th>\n",
       "      <td>4</td>\n",
       "      <td>Muito boa compra em todos sentidos.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28420</th>\n",
       "      <td>5</td>\n",
       "      <td>Entrega feita normalmente. Produto de boa qual...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26173</th>\n",
       "      <td>4</td>\n",
       "      <td>Bom produto. Boa empresa: entregou no prazo co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38062</th>\n",
       "      <td>5</td>\n",
       "      <td>Recomendo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39863</th>\n",
       "      <td>1</td>\n",
       "      <td>S√≥ recebi 01 cadeira, ainda falta uma.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14052</th>\n",
       "      <td>4</td>\n",
       "      <td>Pr√°tico!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       score                                            comment\n",
       "12256      2  O CORREIO ME ENTREGOU SOMENTE UMA CORTINA SEND...\n",
       "25586      3  Recebi parcialmente o pedido. De tr√™s rel√≥gios...\n",
       "29493      5             Bom produto, r√°pida entrega! Recomendo\n",
       "5118       5  Chegou tudo conforme esperado. O que mais surp...\n",
       "21710      4                Muito boa compra em todos sentidos.\n",
       "28420      5  Entrega feita normalmente. Produto de boa qual...\n",
       "26173      4  Bom produto. Boa empresa: entregou no prazo co...\n",
       "38062      5                                         Recomendo.\n",
       "39863      1             S√≥ recebi 01 cadeira, ainda falta uma.\n",
       "14052      4                                           Pr√°tico!"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c64c38d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                Recebi bem antes do prazo estipulado.\n",
       "1    Parab√©ns lojas lannister adorei comprar pela I...\n",
       "2    aparelho eficiente. no site a marca do aparelh...\n",
       "3         Mas um pouco ,travando...pelo valor ta Boa. \n",
       "4    Vendedor confi√°vel, produto ok e entrega antes...\n",
       "Name: comment, dtype: str"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_breaklines(text):\n",
    "    if isinstance(text, str):\n",
    "        # Replace \\r, \\n, and \\r\\n with a space\n",
    "        return re.sub(r'[\\r\\n]+', ' ', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# now lets apply the above function to our dataset\n",
    "\n",
    "df['comment'] = df['comment'].apply(clean_breaklines)\n",
    "\n",
    "df['comment'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391e2b0d",
   "metadata": {},
   "source": [
    "--- Text 1 ---\n",
    "\n",
    "*Before:* \n",
    "*Estava faltando apenas um produto, eu recebi hoje , muito obrigada!*\n",
    "*Tudo certo!*\n",
    "\n",
    "*Att*\n",
    "\n",
    "*Elenice.*\n",
    "\n",
    "*After:*\n",
    "*Estava faltando apenas um produto, eu recebi hoje , muito obrigada!  Tudo certo!    Att     Elenice.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a055f5",
   "metadata": {},
   "source": [
    "Here it's possible to see the tags \\r (_carriage return_ code ASCII 10) and \\n (_new line_ code ASCII 13). With RegEx, we could get rid of those patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8127fd5b",
   "metadata": {},
   "source": [
    "<a id=\"4.2.2\"></a>\n",
    "<font color=\"dimgrey\" size=+1.5><b>Sites and Hiperlinks</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20171cdf",
   "metadata": {},
   "source": [
    "Another pattern that must be threated is sites and hiperlinks. Let's define another function to apply RegEx on this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a964a900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_urls(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove http, https, and www links\n",
    "        return re.sub(r'http\\S+|www\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "df['comment'] = df['comment'].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88aad306",
   "metadata": {},
   "source": [
    "--- Text 1 ---\n",
    "\n",
    "Before: \n",
    "comprei o produto pela cor ilustrada pelo site da loja americana, no site mostra ser preto http://prntscr.com/jkx7hr quando o produto chegou aqui veio todos com a mesma cor, tabaco http://prntscr.com/\n",
    "\n",
    "After: \n",
    "comprei o produto pela cor ilustrada pelo site da loja americana, no site mostra ser preto  link  quando o produto chegou aqui veio todos com a mesma cor, tabaco  link \n",
    "\n",
    "--- Text 2 ---\n",
    "\n",
    "Before: \n",
    "Pedi esse: https://www.lannister.com.br/produto/22880118/botox-capilar-selafix-argan-premium-doux-clair-2x1-litro?pfm_carac=doux%20clair&pfm_index=3&pfm_page=search&pfm_pos=grid&pfm_type=search_page%\n",
    "\n",
    "After: \n",
    "Pedi esse:  link "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0814f682",
   "metadata": {},
   "source": [
    "<a id=\"4.2.3\"></a>\n",
    "<font color=\"dimgrey\" size=+1.5><b>Dates</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8d579b",
   "metadata": {},
   "source": [
    "Well, as long as we are dealing with customers reviews on items bought online, probably date mentions are very common. Let's see some examples and apply a RegEx to change this to `data` (means `date` in english)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5dc0c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dates_and_fix_spaces(text):\n",
    "    if isinstance(text, str):\n",
    "        text = re.sub(r'\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b', '', text)\n",
    "        text = re.sub(r'\\b\\d{4}[/-]\\d{1,2}[/-]\\d{1,2}\\b', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)  # remove extra spaces\n",
    "        return text.strip()\n",
    "    return text\n",
    "\n",
    "df['comment'] = df['comment'].apply(remove_dates_and_fix_spaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7b8640",
   "metadata": {},
   "source": [
    "--- Text 1 ---\n",
    "\n",
    "Before: \n",
    "(tenso) tinhas mais de 10 lojas pra min escolher qual comprar, o pitei pela lannister por ser uma loja conhecida a entrega estava para dia 22/01/2018 . hoje j√° √© 24/01/2018 pois comprei dia 06/01/18\n",
    "\n",
    "After: \n",
    "(tenso) tinhas mais de 10 lojas pra min escolher qual comprar, o pitei pela lannister por ser uma loja conhecida a entrega estava para dia  data  . hoje j√° √©  data  pois comprei dia  data \n",
    "\n",
    "--- Text 2 ---\n",
    "\n",
    "Before: \n",
    "COMPREI EM 21/03/2018, PG VIA CART√ÉO EM 21/03/2018, NF FOI EMITIDA DIA 27/03/2018, PREVIS√ÉO ENTREGA EM 12/04/2018, HOJE √â 14/04/2018, N√ÉO RECEBI, N√ÉO EST√Å EM TRANSPORTE, ESTOU MUITO PREOCUPADO\n",
    "\n",
    "After: \n",
    "COMPREI EM  data , PG VIA CART√ÉO EM  data , NF FOI EMITIDA DIA  data , PREVIS√ÉO ENTREGA EM  data , HOJE √â  data , N√ÉO RECEBI, N√ÉO EST√Å EM TRANSPORTE, ESTOU MUITO PREOCUPADO\n",
    "\n",
    "--- Text 3 ---\n",
    "\n",
    "Before: \n",
    "J√° comprei v√°rias vezes no site \"lannister\";mas  desta √∫ltima vez,fiz uma compra de um TONER no  04.10.16 e s√≥ prometeram p/ 25.11.16 e ainda n√£o  recebi o produto.\n",
    "\n",
    "After: \n",
    "J√° comprei v√°rias vezes no site \"lannister\";mas  desta √∫ltima vez,fiz uma compra de um TONER no   data  e s√≥ prometeram p/  data  e ainda n√£o  recebi o produto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cae76c0",
   "metadata": {},
   "source": [
    "<a id=\"4.2.4\"></a>\n",
    "<font color=\"dimgrey\" size=+1.5><b>Money</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47671be8",
   "metadata": {},
   "source": [
    "Another pattern that probably is very common on this kind of source is representations of money (R$ _,_). To improve our model, maybe it's a good idea to transform this pattern into a key word like valor (means money or amount in english)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "14d7c811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_money(text):\n",
    "    if isinstance(text, str):\n",
    "        # Match Brazilian real format: R$ optional space, digits, optional thousands sep, comma, decimals\n",
    "        text = re.sub(r'R\\$ ?[\\d\\.\\,]+', 'valor', text)\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "# Apply directly to comment column\n",
    "df['comment'] = df['comment'].apply(replace_money)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df91ca7",
   "metadata": {},
   "source": [
    "--- Text 1 ---\n",
    "\n",
    "Before: \n",
    "Recebi o produto correto, por√©m o valor do produto na NF ficou a menor, R$ 172,00 sendo que comprei a 219,90.  O valor do frete calculado foi R$ 18,90 e veio R$ 93,00.  Gostaria que viesse com correto\n",
    "\n",
    "After: \n",
    "Recebi o produto correto, por√©m o valor do produto na NF ficou a menor,  dinheiro  sendo que comprei a 219,90.  O valor do frete calculado foi  dinheiro  e veio  dinheiro .  Gostaria que viesse com correto\n",
    "\n",
    "--- Text 2 ---\n",
    "\n",
    "Before: \n",
    "Infelizmente, para uma entrega em GRU (Regi√£o Metropolitana da Grande SP) achei bem \"salgado\" o valor do frete cobrado sobre o pre√ßo do produto! Afinal, a mercadoria custou R$26,70 + R$15,11 de frete!\n",
    "\n",
    "After: \n",
    "Infelizmente, para uma entrega em GRU (Regi√£o Metropolitana da Grande SP) achei bem \"salgado\" o valor do frete cobrado sobre o pre√ßo do produto! Afinal, a mercadoria custou  dinheiro  +  dinheiro  de frete!\n",
    "\n",
    "--- Text 3 ---\n",
    "\n",
    "Before: \n",
    "Paguei $48,00 reais de frete e acabei tendo que buscar o pedido no Centro de Distribui√ß√£o dos Correios, por√©m a loja nada tem a ver com o mal servi√ßo prestado pela empresa contrata para entrega.\n",
    "\n",
    "After: \n",
    "Paguei  dinheiro  reais de frete e acabei tendo que buscar o pedido no Centro de Distribui√ß√£o dos Correios, por√©m a loja nada tem a ver com o mal servi√ßo prestado pela empresa contrata para entrega."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9befa5be",
   "metadata": {},
   "source": [
    "<a id=\"4.2.5\"></a>\n",
    "<font color=\"dimgrey\" size=+1.5><b>Numbers</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619f0de5",
   "metadata": {},
   "source": [
    "Here we will try to find numbers on reviews and replace them with another string numero (that means number, in english). We could just replace the numbers with whitespace but maybe this would generated some information loss. Let's see what we've got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "06649170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_numbers(text):\n",
    "    if isinstance(text, str):\n",
    "        # Replace any sequence of digits with 'numero'\n",
    "        text = re.sub(r'\\b\\d+\\b', 'numero', text)\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "# Apply directly to the comment column\n",
    "df['comment'] = df['comment'].apply(replace_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af91f31f",
   "metadata": {},
   "source": [
    "--- Text 1 ---\n",
    "\n",
    "Before: \n",
    "Comprei o produto dia 25 de fevereiro e hoje dia 29 de marco n√£o fora entregue na minha resid√™ncia. N√£o sei se os correios desse Brasil e p√©ssimo ou foi a pr√≥pria loja que demorou postar.\n",
    "\n",
    "After: \n",
    "Comprei o produto dia  numero  de fevereiro e hoje dia  numero  de marco n√£o fora entregue na minha resid√™ncia. N√£o sei se os correios desse Brasil e p√©ssimo ou foi a pr√≥pria loja que demorou postar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2daed1",
   "metadata": {},
   "source": [
    "<a id=\"4.2.6\"></a>\n",
    "<font color=\"dimgrey\" size=+1.5><b>Negation</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a199236b",
   "metadata": {},
   "source": [
    "This session was thought and discussed in a special way. The problem statement is that when we remove the stopwords, probabily we would loose the meaning of some phrases about removing the negation words like n√£o (not), for example. So, because of this, maybe is a good idea to replace some negation words with some common words indicating a negation meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1be61f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of common negation words in Portuguese\n",
    "negation_words = [\n",
    "    'n√£o', 'nunca', 'jamais', 'nem', 'nenhum', 'ningu√©m', 'nenhuma'\n",
    "]\n",
    "\n",
    "# Create a regex pattern to match them as whole words\n",
    "negation_pattern = r'\\b(?:' + '|'.join(negation_words) + r')\\b'\n",
    "\n",
    "def replace_negations(text):\n",
    "    if isinstance(text, str):\n",
    "        # Replace each negation word with 'negacao'\n",
    "        return re.sub(negation_pattern, 'negacao', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "# Apply directly to the comment column\n",
    "df['comment'] = df['comment'].apply(replace_negations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a9eebf",
   "metadata": {},
   "source": [
    "--- Text 1 ---\n",
    "\n",
    "Before: \n",
    "O material √© bom, o problema √© que a bolsa n√£o fecha, n√£o possui z√≠per, √© como uma sacola. Isso me deixou insatisfeita, pois na foto n√£o d√° pra perceber e n√£o h√° informa√ß√£o ou foto interna sobre isso.\n",
    "\n",
    "After: \n",
    "O material √© bom, o problema √© que a bolsa  nega√ß√£o  fecha,  nega√ß√£o  possui z√≠per, √© como uma sacola. Isso me deixou insatisfeita, pois na foto  nega√ß√£o  d√° pra perceber e  nega√ß√£o  h√° informa√ß√£o ou foto interna sobre isso.\n",
    "\n",
    "--- Text 2 ---\n",
    "\n",
    "Before: \n",
    "Meu pedido era para ser entregue at√© dia  data , at√© a presente data ( numero / numero ) a nota fiscal n√£o foi emitida, solicitei v√°rias vezes n√£o obtive retorno, n√£o recomendo esta Loja, nem a lannister!!!!!!\n",
    "\n",
    "After: \n",
    "Meu pedido era para ser entregue at√© dia  data , at√© a presente data ( numero / numero ) a nota fiscal  nega√ß√£o  foi emitida, solicitei v√°rias vezes  nega√ß√£o  obtive retorno,  nega√ß√£o  recomendo esta Loja, nem a lannister!!!!!!\n",
    "\n",
    "--- Text 3 ---\n",
    "\n",
    "Before: \n",
    "OEQUIPAMENTO N√ÉO FUNCIONA. O mini cartao SD nao encaixa e o computador n√£o reconhece quando √© conectado com o cabo USB\n",
    "\n",
    "After: \n",
    "OEQUIPAMENTO  nega√ß√£o  FUNCIONA. O mini cartao SD  nega√ß√£o  encaixa e o computador  nega√ß√£o  reconhece quando √© conectado com o cabo USB\n",
    "\n",
    "--- Text 4 ---\n",
    "\n",
    "Before: \n",
    "Cancelei ha tempos, enviaram mesmo assim e nao estornaram os valores\n",
    "\n",
    "After: \n",
    "Cancelei ha tempos, enviaram mesmo assim e  nega√ß√£o  estornaram os valores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f827af38",
   "metadata": {},
   "source": [
    "<font color=\"dimgrey\" size=+1.5><b>Special Characters</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3638236",
   "metadata": {},
   "source": [
    "The search for special characteres is a really special one because we see a lot of this pattern on online comments. Let's build an RegEx motor to find those ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "51192aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def replace_emojis(text):\n",
    "    if isinstance(text, str):\n",
    "        # Emoji ranges in Unicode\n",
    "        emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # Symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # Transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # Flags\n",
    "            u\"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "            u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols & Pictographs\n",
    "            u\"\\U00002600-\\U000026FF\"  # Misc symbols\n",
    "            \"]+\",\n",
    "            flags=re.UNICODE\n",
    "        )\n",
    "        # Replace all emojis/special symbols with 'emoji'\n",
    "        text = emoji_pattern.sub('emoji', text)\n",
    "\n",
    "        # Optionally remove other weird non-alphanumeric characters\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # keep letters, numbers, underscore, spaces\n",
    "\n",
    "        # Remove extra spaces\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    return text\n",
    "\n",
    "# Apply to comment column\n",
    "df['comment'] = df['comment'].apply(replace_emojis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d23b19f",
   "metadata": {},
   "source": [
    "--- Text 1 ---\n",
    "\n",
    "Before: \n",
    "Este foi o pedido  Balde Com  numero  Pe√ßas - Blocos De Montar  numero  un -  dinheiro  cada ( nega√ß√£o  FOI ENTREGUE)  Vendido e entregue targaryen  Tapete de Eva N¬∫ Letras  numero  Pe√ßas Crian√ßas  numero  un -  dinheiro  (ESTE FOI ENTREG\n",
    "\n",
    "After: \n",
    "Este foi o pedido  Balde Com  numero  Pe√ßas   Blocos De Montar  numero  un    dinheiro  cada   nega√ß√£o  FOI ENTREGUE   Vendido e entregue targaryen  Tapete de Eva N¬∫ Letras  numero  Pe√ßas Crian√ßas  numero  un    dinheiro   ESTE FOI ENTREG\n",
    "\n",
    "--- Text 2 ---\n",
    "\n",
    "Before: \n",
    "Cada vez que compro mais fico satisfeita parab√©ns pela honestidade com seus clientes üëèüëèüëèüëè?\n",
    "\n",
    "After: \n",
    "Cada vez que compro mais fico satisfeita parab√©ns pela honestidade com seus clientes      \n",
    "\n",
    "--- Text 3 ---\n",
    "\n",
    "Before: \n",
    "Comprei o produto, paguei no boleto e s√≥ recebi metade do produto, anunciaram uma coisa √© mandaram outra. Muito insatisfeita üò°üò°üò°\n",
    "\n",
    "After: \n",
    "Comprei o produto  paguei no boleto e s√≥ recebi metade do produto  anunciaram uma coisa √© mandaram outra  Muito insatisfeita  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa232c3b",
   "metadata": {},
   "source": [
    "<font color=\"dimgrey\" size=+1.5><b>Additional Whitespaces</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e48cec0",
   "metadata": {},
   "source": [
    "After all the steps we have taken over here, it's important to clean our text eliminating unecessary whitespaces. Let's apply a RegEx for this and see what we've got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d7dcae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_whitespaces(text):\n",
    "    if isinstance(text, str):\n",
    "        # Replace multiple spaces/tabs/newlines with a single space\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        # Remove leading and trailing spaces\n",
    "        return text.strip()\n",
    "    return text\n",
    "\n",
    "# Apply directly to comment column\n",
    "df['comment'] = df['comment'].apply(clean_whitespaces)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3006ad",
   "metadata": {},
   "source": [
    "--- Text 1 ---\n",
    "\n",
    "Before: \n",
    "Mas um pouco  travando   pelo valor ta Boa   \n",
    "\n",
    "After: \n",
    "Mas um pouco travando pelo valor ta Boa\n",
    "\n",
    "--- Text 2 ---\n",
    "\n",
    "Before: \n",
    "Vendedor confi√°vel  produto ok e entrega antes do prazo \n",
    "\n",
    "After: \n",
    "Vendedor confi√°vel produto ok e entrega antes do prazo\n",
    "\n",
    "--- Text 3 ---\n",
    "\n",
    "Before: \n",
    "meu produto chegou e ja tenho que devolver  pois est√° com defeito    nega√ß√£o  segurar carga\n",
    "\n",
    "After: \n",
    "meu produto chegou e ja tenho que devolver pois est√° com defeito nega√ß√£o segurar carga\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd350545",
   "metadata": {},
   "source": [
    "<font color=\"dimgrey\" size=+2.0><b>Stopwords</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32213ea6",
   "metadata": {},
   "source": [
    "Well, by now we have a text dataset without any pattern that we threated with RegEx and also without punctuations. In other words, we have a half-clean text with a rich transformation applied. \n",
    "\n",
    "So, we are ready to apply some advanced text transformations like `stopwords` removal, `stemming` and the `TF-IDF` matrix process. Let's start with portuguese stopwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f4354c",
   "metadata": {},
   "source": [
    "Step 1 : Import Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0464f48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Portuguese stopwords: 207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ayush-\n",
      "[nltk_data]     wase/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download Portuguese stopwords if not already done\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load Portuguese stopwords\n",
    "pt_stopwords = set(stopwords.words('portuguese'))\n",
    "print(f'Total Portuguese stopwords: {len(pt_stopwords)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1468d8",
   "metadata": {},
   "source": [
    "Step 2 : Write a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4aade9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):\n",
    "        # Split text into words\n",
    "        words = text.split()\n",
    "        # Keep only words not in stopwords\n",
    "        words = [word for word in words if word.lower() not in pt_stopwords]\n",
    "        # Rejoin into cleaned text\n",
    "        return ' '.join(words)\n",
    "    return text\n",
    "\n",
    "# now apply to our dataframe\n",
    "df['comment'] = df['comment'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc7fbbd",
   "metadata": {},
   "source": [
    "--- Text 1 ---\n",
    "\n",
    "Before: \n",
    "Recebi bem antes do prazo estipulado\n",
    "\n",
    "After: \n",
    "recebi bem antes prazo estipulado\n",
    "\n",
    "--- Text 2 ---\n",
    "\n",
    "Before: \n",
    "Este foi o pedido Balde Com numero Pe√ßas Blocos De Montar numero un dinheiro cada nega√ß√£o FOI ENTREGUE Vendido e entregue targaryen Tapete de Eva N¬∫ Letras numero Pe√ßas Crian√ßas numero un dinheiro ESTE FOI ENTREG\n",
    "\n",
    "After: \n",
    "pedido balde numero pe√ßas blocos montar numero un dinheiro cada nega√ß√£o entregue vendido entregue targaryen tapete eva n¬∫ letras numero pe√ßas crian√ßas numero un dinheiro entreg\n",
    "\n",
    "--- Text 3 ---\n",
    "\n",
    "Before: \n",
    "O produto nega√ß√£o √© bom\n",
    "\n",
    "After: \n",
    "produto nega√ß√£o bom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2d9932",
   "metadata": {},
   "source": [
    "<font color=\"dimgrey\" size=+2.0><b>Stemming</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a1c68",
   "metadata": {},
   "source": [
    "Let's define a function to apply the stemming process on the comments. We will also give examples of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408abc27",
   "metadata": {},
   "source": [
    "Step 1 : Import and initialize the stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e3767e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to /home/ayush-wase/nltk_data...\n",
      "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RSLPStemmer\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data (if not done yet)\n",
    "nltk.download('rslp')\n",
    "\n",
    "# Initialize Portuguese stemmer\n",
    "stemmer = RSLPStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8598ea57",
   "metadata": {},
   "source": [
    "Step 2 : Define a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "23d84765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_stemming(text):\n",
    "    if isinstance(text, str):\n",
    "        # Split text into words\n",
    "        words = text.split()\n",
    "        # Apply stemmer to each word\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]\n",
    "        # Rejoin back into a single string\n",
    "        return ' '.join(stemmed_words)\n",
    "    return text\n",
    "\n",
    "\n",
    "# apply to our dataframe\n",
    "\n",
    "df['comment'] = df['comment'].apply(apply_stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc8b417",
   "metadata": {},
   "source": [
    "--- Text 1 ---\n",
    "\n",
    "Before: \n",
    "recebi bem antes prazo estipulado\n",
    "\n",
    "After: \n",
    "receb bem ant praz estipul\n",
    "\n",
    "--- Text 2 ---\n",
    "\n",
    "Before: \n",
    "pedido balde numero pe√ßas blocos montar numero un dinheiro cada nega√ß√£o entregue vendido entregue targaryen tapete eva n¬∫ letras numero pe√ßas crian√ßas numero un dinheiro entreg\n",
    "\n",
    "After: \n",
    "ped bald numer pe√ß bloc mont numer un dinh cad neg entreg vend entreg targaryen tapet eva n¬∫ letr numer pe√ß crian√ß numer un dinh entreg\n",
    "\n",
    "--- Text 3 ---\n",
    "\n",
    "Before: \n",
    "produto chegou ja devolver pois defeito nega√ß√£o segurar carga\n",
    "\n",
    "After: \n",
    "produt cheg ja devolv poi defeit neg segur carg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892db288",
   "metadata": {},
   "source": [
    "<font color=\"dimgrey\" size=+1.5><b>TF-IDF</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3521e3",
   "metadata": {},
   "source": [
    "With the _Bag of Words_ approach, each words has the same weight, wich maybe can't be true all the time, mainly for those words with a really low frequency on the corpus. So, the _TF-IDF (Term Frequency and Inverse Document Frequency)_ approach can be used with the scikit-learn library following the formulas:\n",
    "\n",
    "$$TF=\\frac{\\text{Frequency of a word in the document}}{\\text{Total words in the document}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b87a389",
   "metadata": {},
   "source": [
    "$$IDF = \\log\\left({\\frac{\\text{Total number of docs}}{\\text{Number of docs containing the words}}}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b321bf",
   "metadata": {},
   "source": [
    "Step 1 ‚Äî Import TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c888442d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303409e7",
   "metadata": {},
   "source": [
    "Step 2 ‚Äî Initialize the vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8cbf535a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=1000,  # Keep top 1000 words (to limit dimensionality)\n",
    "    lowercase=True,     # Ensure all text is lowercase\n",
    "    stop_words=None     # Already removed manually\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f7b48c",
   "metadata": {},
   "source": [
    "Step 3 ‚Äî Fit and transform the stemmed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc657a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the cleaned and stemmed comments directly\n",
    "corpus = df['comment'].tolist()  # convert column to list\n",
    "\n",
    "# Fit TF-IDF and transform\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1a1ce8a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>220v</th>\n",
       "      <th>abaix</th>\n",
       "      <th>abert</th>\n",
       "      <th>abr</th>\n",
       "      <th>abra√ß</th>\n",
       "      <th>absurd</th>\n",
       "      <th>acab</th>\n",
       "      <th>aceit</th>\n",
       "      <th>acess</th>\n",
       "      <th>ach</th>\n",
       "      <th>...</th>\n",
       "      <th>v√£o</th>\n",
       "      <th>whey</th>\n",
       "      <th>zer</th>\n",
       "      <th>√°gil</th>\n",
       "      <th>√°gu</th>\n",
       "      <th>√≥tim</th>\n",
       "      <th>√∫lt</th>\n",
       "      <th>√∫nic</th>\n",
       "      <th>√∫tel</th>\n",
       "      <th>√∫til</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   220v  abaix  abert  abr  abra√ß  absurd  acab  aceit  acess  ach  ...  v√£o  \\\n",
       "0   0.0    0.0    0.0  0.0    0.0     0.0   0.0    0.0    0.0  0.0  ...  0.0   \n",
       "1   0.0    0.0    0.0  0.0    0.0     0.0   0.0    0.0    0.0  0.0  ...  0.0   \n",
       "2   0.0    0.0    0.0  0.0    0.0     0.0   0.0    0.0    0.0  0.0  ...  0.0   \n",
       "3   0.0    0.0    0.0  0.0    0.0     0.0   0.0    0.0    0.0  0.0  ...  0.0   \n",
       "4   0.0    0.0    0.0  0.0    0.0     0.0   0.0    0.0    0.0  0.0  ...  0.0   \n",
       "\n",
       "   whey  zer  √°gil  √°gu  √≥tim  √∫lt  √∫nic  √∫tel  √∫til  \n",
       "0   0.0  0.0   0.0  0.0   0.0  0.0   0.0   0.0   0.0  \n",
       "1   0.0  0.0   0.0  0.0   0.0  0.0   0.0   0.0   0.0  \n",
       "2   0.0  0.0   0.0  0.0   0.0  0.0   0.0   0.0   0.0  \n",
       "3   0.0  0.0   0.0  0.0   0.0  0.0   0.0   0.0   0.0  \n",
       "4   0.0  0.0   0.0  0.0   0.0  0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Feature names (words)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert to DataFrame for readability\n",
    "df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=feature_names)\n",
    "\n",
    "# Inspect the first few rows\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449033fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
